{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0928a6",
   "metadata": {},
   "source": [
    "### Composite AGN Cutout Query Creator\n",
    "This notebook will create `SQL` query requests for a series of cutout images of a specific single quasar to create training data for the transfer-learning phase on real data for `DualFinder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c18ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, cm\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import glob\n",
    "import scipy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "from astropy.table import Table\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import requests\n",
    "import time\n",
    "import tarfile\n",
    "from schwimmbad import MultiPool\n",
    "import multiprocess as mp\n",
    "from astropy.io import fits\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "from os.path import exists\n",
    "import shutil\n",
    "import requests\n",
    "from offset_AGN_dataset_creator import DatasetCreator\n",
    "import sys\n",
    "sys.path.append(\"data_preprocessing/\")\n",
    "sys.path.append(\"data_preprocessing/training_datasets/\")\n",
    "from FITSViewer import display_images_with_buttons\n",
    "#from dual_AGN_utils import crop_center\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a753c33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vast/palmer/scratch/urry/iam37/Project_DRAGON/DRAGON_CNN\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "#os.makedirs(\"Rotated_AGN_diff_fltrs/HSC_R/\")\n",
    "#os.makedirs(\"Rotated_AGN_diff_fltrs/HSC_G/\")\n",
    "#os.makedirs(\"Rotated_AGN_diff_fltrs/HSC_Z/\")\n",
    "#os.makedirs(\"Rotated_AGN_diff_fltrs/HSC_Y/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d3caae-2e39-479f-8957-190a0e8e0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, cropx, cropy):\n",
    "    \n",
    "    #Function from \n",
    "    #https://stackoverflow.com/questions/39382412/crop-center-portion-of-a-numpy-image\n",
    "    \n",
    "    y, x, *_ = img.shape\n",
    "    startx = x // 2 - (cropx // 2)\n",
    "    #print(startx)\n",
    "    starty = y // 2 - (cropy // 2) \n",
    "    #print(starty)\n",
    "    return img[starty:starty + cropy, startx:startx + cropx, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6a9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgdwnldr_gen(args):\n",
    "    \n",
    "    df,num_start,num_stop,fltr = args\n",
    "    \n",
    "    t = Table()\n",
    "    t[\"rerun\"] = [\"pdr3_wide\"]*len(df[num_start:num_stop])\n",
    "    t[\"filter\"] = [fltr]*len(df[num_start:num_stop])\n",
    "    #t[\"tract\"] = df[\"tract\"][num_start:num_stop]\n",
    "    t[\"ra\"] = df[\"ra\"][num_start:num_stop]\n",
    "    t[\"dec\"] = df[\"dec\"][num_start:num_stop]\n",
    "    t[\"type\"] = [\"coadd\"]*len(df[num_start:num_stop])\n",
    "    t[\"sh\"] = [\"16asec\"]*len(df[num_start:num_stop])\n",
    "    t[\"sw\"] = [\"16asec\"]*len(df[num_start:num_stop])\n",
    "    t[\"name\"] = df[\"name\"][num_start:num_stop]\n",
    "                  \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a6cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_writer(args):\n",
    "    \n",
    "    table, i, filepath_prefix, overwrite, tableformat, comment= args\n",
    "    #os.chdir(filepath_prefix)\n",
    "    #print(\"CURRENT WORKING DIRECTORY: \" + str(os.getcwd()))\n",
    "    table.write(filepath_prefix + \"download_sql\" + \".txt\",\n",
    "                overwrite=overwrite,format=tableformat,\n",
    "                comment=comment)\n",
    "    #os.chdir(\"/Users/moskowitzi/Library/CloudStorage/Dropbox/First_Year_at_Yale/Summer_2022/Quasar_Research\")\n",
    "#args = (test2DF, 0, 35, \"HSC-R\")\n",
    "#secondTestTable = imgdwnldr_gen(args)\n",
    "#filepath_prefix = 'input_second_test_HSC_new'\n",
    "def write_dwnldr_files(df,step,filepath_prefix = None,write=True,\n",
    "                      overwrite=True, fltr = None):   \n",
    "                  \n",
    "    len_df = len(df)\n",
    "    iters = int(len_df/step)\n",
    "    #print(len_df)\n",
    "    lwr_ends = range(0,len_df,step)\n",
    "    upr_ends = range(step,len_df+step,step)\n",
    "    fltrs = [fltr]*(iters+1)\n",
    "    dfs = [df]*(iters+1)\n",
    "    \n",
    "    args_imgdwnldr_gen = list(zip(dfs,lwr_ends,upr_ends,fltrs))\n",
    "    #args_imgdwnldr_gen = (dfs,lwr_ends,upr_ends,fltrs)\n",
    "    #imgdwnldr_gen(args_imgdwnldr_gen)\n",
    "    with mp.Pool() as pool:\n",
    "        tables = list(tqdm(pool.imap(imgdwnldr_gen,args_imgdwnldr_gen), total = iters+1))\n",
    "    \n",
    "    if write is True:\n",
    "        paths = [filepath_prefix]*(iters+1)\n",
    "        overwrites = [overwrite]*(iters+1)\n",
    "        tableformats = ['ascii.commented_header']*(iters+1)\n",
    "        comments = [\"#?\"]*(iters+1)\n",
    "        i_s = np.arange(iters+1)\n",
    "    \n",
    "        args_table_writer = list(zip(tables,i_s,paths,overwrites,tableformats,comments))\n",
    "        #args_table_writer = (tables,i_s,paths,overwrites,tableformats,comments)\n",
    "        #table_writer(args_table_writer)\n",
    "        \n",
    "        with mp.Pool() as pool:\n",
    "            exit_codes = list(tqdm(pool.imap(table_writer,args_table_writer), total = iters+1))\n",
    "        #print(\"got here after table writer\")\n",
    "        \n",
    "    else:\n",
    "        for table in tables:\n",
    "            print(table)\n",
    "    return tables\n",
    "def write_rotation_test_coordinates(quasarName, separation, raStart, decStart, fltr):\n",
    "    count = 0\n",
    "    degree_separation = separation/3600\n",
    "    raList = []\n",
    "    decList = []\n",
    "    quasarNameList = []\n",
    "    #random_angles = np.random.uniform(250, 290, 50)\n",
    "    random_angles = np.random.uniform(0.0, 360.0, 200)\n",
    "    for i in random_angles:\n",
    "        ra = raStart + degree_separation* np.cos(i)\n",
    "        dec = decStart + degree_separation* np.sin(i)\n",
    "        raList.append(ra)\n",
    "        decList.append(dec)\n",
    "        quasarNameNew = quasarName + \"_HSC_\" + str(fltr) + \"_rotated_\" + str(i) + \"_degrees\"\n",
    "        #print(quasarNameNew)\n",
    "        quasarNameList.append(quasarNameNew)\n",
    "    return raList, decList, quasarNameList\n",
    "#Automatic downloaders for downloading from HSC servers:\n",
    "def cutout_requester(args):\n",
    "    \n",
    "    session, listpath_prefix, write_path_prefix, i = args\n",
    "    print(f\"listpath_prefix: {listpath_prefix}\")\n",
    "    print(f\"write_path_prefix: {write_path_prefix}\")\n",
    "    \n",
    "    files = {\n",
    "        'list': (listpath_prefix + \"download_sql.txt\", open(listpath_prefix + \"download_sql.txt\", 'rb')),\n",
    "    }\n",
    "    \n",
    "    with session.post('https://hsc-release.mtk.nao.ac.jp/das_cutout/pdr3/cgi-bin/cutout', \n",
    "                      files=files, auth=('moskowitzi', 'UCiNwD3CHzbprC8G1QM2aUxpQIZdp2cOjyQWOO7J'), stream=True) as response:\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        chunk_size = len(i)  # Define chunk size\n",
    "        \n",
    "        with open(write_path_prefix + \"download.tar\", 'wb') as f, tqdm(\n",
    "            total=total_size, unit='B', unit_scale=True, desc=write_path_prefix + \"download.tar\") as progress_bar:\n",
    "            \n",
    "            for chunk in response.iter_content(chunk_size):\n",
    "                if chunk:  # Filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "                    progress_bar.update(len(chunk))\n",
    "def members(tf):\n",
    "    \n",
    "    for i, member in enumerate(tf.getmembers()):\n",
    "        if i == 0:\n",
    "            l = len(member.path.split(\"/\")[0]) + 1\n",
    "        \n",
    "        member.path = member.path[l:]\n",
    "        yield member\n",
    "def write_band_download_cutouts(quasarNameList, raList, decList, fltr, filepath, filepath_prefix, separation):\n",
    "    \n",
    "    header = ['ra', 'dec', 'name']\n",
    "    print(len(raList))\n",
    "    #if not (exists(filepath) or exists(filepath_prefix)):\n",
    "        #os.makedirs(filepath)\n",
    "        #os.makedirs(filepath_prefix)\n",
    "    if not exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    if not exists(filepath_prefix):\n",
    "        os.makedirs(filepath_prefix)\n",
    "    with open(filepath + \"catalog_\" + str(separation) + \".csv\", 'w', encoding = 'UTF8') as f: \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for j in range(len(raList)):\n",
    "            #print(\"raList[j]: \" + str(raList[j]))\n",
    "            #print(type(raList[j]))\n",
    "            combinedData = [raList[j], decList[j], quasarNameList[j]]\n",
    "            writer.writerow(combinedData)\n",
    "    #print(\"got here\")\n",
    "    test_df = pd.read_csv(filepath + \"catalog_\" + str(separation) + \".csv\", delimiter = ',')\n",
    "    write = True\n",
    "    overwrite = True\n",
    "    #print(test_df)\n",
    "    write_dwnldr_files(test_df, fltr = fltr, step=len(raList), write=write,overwrite=overwrite,\n",
    "                filepath_prefix = filepath_prefix)\n",
    "        \n",
    "#Function to actuall extract all the tarballs\n",
    "def extractor(args):\n",
    "    tar_file_prefix, write_path, i = args\n",
    "    \n",
    "    with tarfile.open(tar_file_prefix + f\"download.tar\") as tf:\n",
    "        tf.extractall(write_path,members=members(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecded443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducedAmplitudeTestFile(quasarName, percentage, separation, raStart, decStart, fltr, filepath = None):\n",
    "    #quasarName = quasarName\n",
    "    print(quasarName)\n",
    "    \n",
    "    #raStart = 154.60814662744912\n",
    "    #decStart = 0.34943320872888634\n",
    "    raList, decList, quasarNameList = write_rotation_test_coordinates(quasarName, separation, raStart, decStart, fltr)\n",
    "    #print(\"got here\")\n",
    "    header = ['ra', 'dec', 'name']\n",
    "#args = secondTestTable, filepath_prefix, overwrite, tableformat, comment\n",
    "#table_writer(args)\n",
    "    #print(len(quasarNameList))\n",
    "    #print(len(raList))\n",
    "    #print(len(decList))\n",
    "    if not exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    with open(filepath + \"rotation_test_reduced_amplitude_\"+str(separation)+\".csv\", 'w', encoding = 'UTF8') as f: \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for j in range(len(raList)):\n",
    "            combinedData = [raList[j], decList[j], quasarNameList[j]]\n",
    "            writer.writerow(combinedData)\n",
    "    #print(\"got here\")\n",
    "    test_df = pd.read_csv(filepath + \"rotation_test_reduced_amplitude_\"+str(separation)+\".csv\", delimiter = ',')\n",
    "    write = True\n",
    "    overwrite = True\n",
    "    if not exists(filepath + \"downloader_files/downloader_for_\" + str((separation)) + \"/\"):\n",
    "        os.makedirs(filepath + \"downloader_files/downloader_for_\" + str((separation)) + \"/\")\n",
    "    tables = write_dwnldr_files(test_df,step=len(raList), fltr = fltr, write=write,overwrite=overwrite,\n",
    "                filepath_prefix= filepath + \"downloader_files/downloader_for_\" + str((separation)) + \"/\")\n",
    "    #print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c34b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = fits.getdata(\"cutout-HSC-G-9226-pdr3_wide-240530-152449.fits\")\n",
    "plt.imshow(img, vmin = np.percentile(img, 1), vmax = np.percentile(img, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/vast/palmer/scratch/urry/iam37/\")\n",
    "#All Spring Equitorial Quasars in SDSS and HSC\n",
    "starttime = time.time()\n",
    "fits_table_dr16q = '../DR16Q_v4.fits'\n",
    "hdu1 = fits.open(fits_table_dr16q)\n",
    "data = hdu1[1].data\n",
    "min_row_fall_1 = 668635\n",
    "max_row_fall_1 = 750414\n",
    "min_row_fall_2 = 1\n",
    "max_row_fall_2 = 116727\n",
    "quasarNameList_fall = []\n",
    "quasarRAList_fall = []\n",
    "quasarDecList_fall = []\n",
    "quasarZList_fall = []\n",
    "\"\"\"Fall Equatorial Band\"\"\"\n",
    "for j in tqdm(range(min_row_fall_1, max_row_fall_2)):\n",
    "    #if(data[j].field(2) >= -1.0 and data[j].field(2) <= 7.0 and (data[j].field(6) == \"QSO\" or data[j].field(7) == \"QSO\" or data[j].field(6) == data[j].field(7) == \"UNK\")):\n",
    "    if(data[j].field(2) >= -1.0 and data[j].field(2) <= 7.0 and (data[j].field(6) == \"QSO\" or data[j].field(7) == \"QSO\" or data[j].field(6) == data[j].field(7) == \"UNK\") and data[j].field(28)):\n",
    "    \n",
    "        quasarName = data[j].field(6) + \" \" + data[j].field(0)\n",
    "        quasarNameNoSpace = quasarName.replace(\" \", \"_\")\n",
    "        #quasarNameNoSpaceNoPeriod = quasarNameNoSpace.replace(\".\", \"_\")\n",
    "        #quasarNameNoPlus = quasarNameNoSpaceNoPeriod.replace(\"+\", \"_\")\n",
    "        inputQuasarName = quasarNameNoSpace\n",
    "        quasarNameList_fall.append(inputQuasarName)\n",
    "        quasarRAList_fall.append(data[j].field(1))\n",
    "        quasarDecList_fall.append(data[j].field(2))\n",
    "        quasarZList_fall.append(data[j].field(28))\n",
    "for j in tqdm(range(min_row_fall_2, max_row_fall_2)):\n",
    "    if(data[j].field(2) >= -1.0 and data[j].field(2) <= 7.0 and (data[j].field(6) == \"QSO\" or data[j].field(7) == \"QSO\" or data[j].field(6) == data[j].field(7) == \"UNK\") and data[j].field(28)<=1.59):\n",
    "        quasarName = data[j].field(6) + \" \" + data[j].field(0)\n",
    "        quasarNameNoSpace = quasarName.replace(\" \", \"_\")\n",
    "        inputQuasarName = quasarNameNoSpace\n",
    "        quasarNameList_fall.append(inputQuasarName)\n",
    "        quasarRAList_fall.append(data[j].field(1))\n",
    "        quasarDecList_fall.append(data[j].field(2))\n",
    "        quasarZList_fall.append(data[j].field(28))\n",
    "quasarNameList_fall = np.asarray(quasarNameList_fall)\n",
    "#for j in range(len(quasarNameList_fall)):\n",
    "    #if quasarNameList_fall[j].find(\"QSO\") == -1:\n",
    "        #print(quasarNameList_fall[j])\n",
    "        #print(j)\n",
    "print(len(quasarNameList_fall))\n",
    "print(len(quasarDecList_fall))\n",
    "header = ['ra', 'dec', 'Quasar name', 'Z']\n",
    "#print(len(quasarNameList))\n",
    "if not exists(\"HSC_survey_bands/\"):\n",
    "    os.makedirs(\"HSC_survey_bands/\")\n",
    "with open(\"HSC_survey_bands/fall_equitorial_quasar_list.csv\", 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for i in range(0, len(quasarNameList_fall)):\n",
    "        combinedData = [quasarRAList_fall[i], quasarDecList_fall[i], quasarNameList_fall[i], quasarZList_fall[i]]\n",
    "        writer.writerow(combinedData)\n",
    "fall_df = pd.read_csv(\"HSC_survey_bands/fall_equitorial_quasar_list.csv\", delimiter=',')\n",
    "#print(fall_df)\n",
    "\"\"\"Spring Equatorial Band\"\"\"\n",
    "valid_rows1 = data[170000:552181]\n",
    "print(data[0][0])\n",
    "print(data[0].field(2))\n",
    "quasarNameList_spring = []\n",
    "quasarRAList = []\n",
    "quasarDecList = []\n",
    "quasarZList = []\n",
    "#We know that HSC's field contains RA = 130º at 199012, I will now test smaller RA's to see if they are also contained within the \n",
    "#survey field\n",
    "for i in tqdm(range(191617, 552181)):\n",
    "    if (data[i].field(2)>= -7.0 and data[i].field(2) <= 1.0 and (data[i].field(6) == \"QSO\" or data[i].field(7) == \"QSO\" or data[i].field(6) == data[i].field(7) == \"UNK\") and data[i].field(28)<=1.59):\n",
    "        quasarName = data[i].field(6) + \" \" + data[i].field(0)\n",
    "        quasarNameNoSpace = quasarName.replace(\" \", \"_\")\n",
    "        inputQuasarName = quasarNameNoSpace\n",
    "        quasarNameList_spring.append(inputQuasarName)\n",
    "        quasarRAList.append(data[i].field(1))\n",
    "        quasarDecList.append(data[i].field(2))\n",
    "        quasarZList.append(data[i].field(28))\n",
    "quasarNameList_spring = np.asarray(quasarNameList_spring)\n",
    "#for j in range(len(quasarNameList_spring)):\n",
    "    #if quasarNameList_spring[j].find(\"QSO\") == -1:\n",
    "        #print(quasarNameList_spring[j])\n",
    "        #print(\"Not a QSO Object\")\n",
    "        #print(j)\n",
    "#header = ['ra', 'dec', 'Quasar name']\n",
    "print(len(quasarNameList_spring))\n",
    "with open(\"HSC_survey_bands/spring_equitorial_quasar_list.csv\", 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for i in range(0, len(quasarNameList_spring)):\n",
    "        combinedData = [quasarRAList[i], quasarDecList[i], quasarNameList_spring[i], quasarZList[i]]\n",
    "        writer.writerow(combinedData)\n",
    "spring_df = pd.read_csv(\"HSC_survey_bands/spring_equitorial_quasar_list.csv\", delimiter=',')\n",
    "#print(spring_df)\n",
    "#for i in range(0, len(spring_df)):\n",
    "    #numpyVersion = spring_df.loc[i].to_numpy()\n",
    "    #print(numpyVersion)\n",
    "    #if (numpyVersion[1] <=-6.0):\n",
    "        #print(\"Something went wrong here\")\n",
    "        #print(spring_df.loc[i])\n",
    "#display(spring_df.to_string())\n",
    "\"\"\"\n",
    "with open(\"spring_equitorial_quasar_list_hsc_download.csv\", \"w\", encoding='UTF8') as p:\n",
    "    writer = csv.writer(p)\n",
    "    header = [\"#?\", \"rerun\", \"filter\", \"ra\", \"dec\", \"sw\", \"sh\", \"name\"]\n",
    "    writer.writerow(header)\n",
    "    for i in range(0, len(quasarNameList)):\n",
    "        combinedData = [NaN, \"pdr3_wide\", \"HSC-R\", quasarRAList[i], quasarDecList[i], \"0.01asec\", \"0.01asec\", \"spring_equitorial_\"+str(i)]\n",
    "        writer.writerow(combinedData)\n",
    "\"\"\"\n",
    "#print(quasarNameList)\n",
    "#print(quasarZList)\n",
    "#In this section of the download, we are working in the Spring Equitorial survey path, which goes from roughly\n",
    "#120 degrees to 225 degrees of RA and -7 to 1 degrees of Dec\n",
    "\"\"\"Northern Sky Band\"\"\"\n",
    "min_row_north = 448834\n",
    "max_row_north = 602111\n",
    "quasarNameList_north = []\n",
    "quasarRAList_north = []\n",
    "quasarDecList_north = []\n",
    "quasarZList_north = []\n",
    "for k in tqdm(range(min_row_north, max_row_north)):\n",
    "    if(data[k].field(2) >= 42.5 and data[k].field(2) <= 44.0 and (data[k].field(6) == \"QSO\" or data[k].field(7) == \"QSO\" or data[k].field(6) == data[k].field(7) == \"UNK\") and data[k].field(28)<=1.59):\n",
    "        quasarName = data[k].field(6) + \" \" + data[k].field(0)\n",
    "        quasarNameNoSpace = quasarName.replace(\" \", \"_\")\n",
    "        inputQuasarName = quasarNameNoSpace\n",
    "        quasarNameList_north.append(inputQuasarName)\n",
    "        quasarRAList_north.append(data[k].field(1))\n",
    "        quasarDecList_north.append(data[k].field(2))\n",
    "        quasarZList_north.append(data[k].field(28))\n",
    "quasarNameList_north = np.asarray(quasarNameList_north)\n",
    "#for k in range(len(quasarNameList_north)):\n",
    "    #if quasarNameList_north[k].find(\"QSO\") == -1:\n",
    "        #print(quasarNameList_north[k])\n",
    "        #print(k)\n",
    "print(len(quasarNameList_north))\n",
    "print(\"Total number of QSO objects = \" + str(len(quasarNameList_fall) + len(quasarNameList_spring) + len(quasarNameList_north)))\n",
    "with open(\"HSC_survey_bands/northern_sky_quasar_list.csv\", 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for i in range(0, len(quasarNameList_north)):\n",
    "        combinedData = [quasarRAList_north[i], quasarDecList_north[i], quasarNameList_north[i], quasarZList_north[i]]\n",
    "        writer.writerow(combinedData)\n",
    "northern_df = pd.read_csv(\"HSC_survey_bands/northern_sky_quasar_list.csv\", delimiter=',')\n",
    "endtime = time.time()\n",
    "print(\"--- %s seconds ---\" % (endtime - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1ccca-de9d-4f67-b7d4-c26aafca7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds 'STAR' objects in the SDSS catalogue instead of just QSO objects.\n",
    "start_time = time.time()\n",
    "fits_table_dr16q = '../DR16Q_v4.fits'\n",
    "hdu1 = fits.open(fits_table_dr16q)\n",
    "data = hdu1[1].data\n",
    "# Define constants\n",
    "min_row_fall_1 = 668635\n",
    "max_row_fall_1 = 750414\n",
    "min_row_fall_2 = 1\n",
    "max_row_fall_2 = 116727\n",
    "header = ['ra', 'dec', 'Object name', 'Z']\n",
    "# Function to process data\n",
    "def process_data(data, min_row, max_row, ra_range, dec_range, filename, object_type='STAR'):\n",
    "    object_name_list = []\n",
    "    ra_list = []\n",
    "    dec_list = []\n",
    "    z_list = []\n",
    "\n",
    "    for j in tqdm(range(min_row, max_row)):\n",
    "        if ra_range[0] <= data[j].field(2) <= ra_range[1] and (data[j].field(6) == object_type or data[j].field(7) == object_type or data[j].field(6) == data[j].field(7) == object_type):\n",
    "            object_name = data[j].field(6) + \" \" + data[j].field(0)\n",
    "            object_name_no_space = object_name.replace(\" \", \"_\")\n",
    "            object_name_list.append(object_name_no_space)\n",
    "            ra_list.append(data[j].field(1))\n",
    "            dec_list.append(data[j].field(2))\n",
    "            z_list.append(data[j].field(28))\n",
    "\n",
    "    # Save to CSV\n",
    "    if not os.path.exists(\"HSC_survey_bands/\"):\n",
    "        os.makedirs(\"HSC_survey_bands/\")\n",
    "\n",
    "    with open(f\"HSC_survey_bands/{filename}\", 'w', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for i in range(len(object_name_list)):\n",
    "            combined_data = [ra_list[i], dec_list[i], object_name_list[i], z_list[i]]\n",
    "            writer.writerow(combined_data)\n",
    "\n",
    "    return len(object_name_list)\n",
    "\n",
    "# Process Fall Equatorial Band\n",
    "fall_count = process_data(data, min_row_fall_1, max_row_fall_1, (-1.0, 7.0), (-1.0, 7.0), 'fall_equitorial_object_list.csv', 'STAR')\n",
    "fall_count += process_data(data, min_row_fall_2, max_row_fall_2, (-1.0, 7.0), (-1.0, 7.0), 'fall_equitorial_object_list.csv', 'STAR')\n",
    "\n",
    "# Process Spring Equatorial Band\n",
    "spring_count = process_data(data, 191617, 552181, (-7.0, 1.0), (-7.0, 1.0), 'spring_equitorial_object_list.csv', 'STAR')\n",
    "\n",
    "# Process Northern Sky Band\n",
    "north_count = process_data(data, 448834, 602111, (42.5, 44.0), (42.5, 44.0), 'northern_sky_object_list.csv', 'STAR')\n",
    "\n",
    "fall_df = pd.read_csv(\"HSC_survey_bands/fall_equitorial_object_list.csv\", delimiter=',')\n",
    "spring_df = pd.read_csv(\"HSC_survey_bands/spring_equitorial_object_list.csv\", delimiter=',')\n",
    "northern_df = pd.read_csv(\"HSC_survey_bands/northern_sky_object_list.csv\", delimiter=',')\n",
    "\n",
    "# Print results\n",
    "print(f\"Fall count: {fall_count}\")\n",
    "print(f\"Spring count: {spring_count}\")\n",
    "print(f\"Northern count: {north_count}\")\n",
    "print(f\"Total number of STAR objects: {fall_count + spring_count + north_count}\")\n",
    "\n",
    "# End timer\n",
    "endtime = time.time()\n",
    "print(\"--- %s seconds ---\" % (endtime - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc437c13-fa7c-4f42-b8aa-e0ec7daff27d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(spring_df.head(20))\n",
    "print(fall_df.head(20))\n",
    "print(northern_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb0e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "percentages = np.arange(0.1, 1.0, 0.1)\n",
    "separations_distribution_1 = np.random.uniform(0.90, 2.5, 200)\n",
    "#angle = np.random.uniform(0.0, 359.0, 200)\n",
    "#quasar_name_1 = \"QSO_000035.59-003146.1\"\n",
    "#quasar_name_2 = \"QSO_131227.61-002134.3\"\n",
    "#ra_start_1 = 0.14833262219804055 + 0.67 * np.cos(0)/3600.0\n",
    "#dec_start_1 = -0.5294901735587073\n",
    "#ra_start_2 = 198.11505695798235 + 0.67 * np.cos(0)/3600.0\n",
    "#dec_start_2 = -0.3595408479420831\n",
    "\n",
    "rotation_AGN_candidate_images = []\n",
    "rotation_AGN_candidate_names = []\n",
    "rotation_ra_list = []\n",
    "rotation_dec_list = []\n",
    "for ii, confirmed_single in tqdm(enumerate(glob.glob(\"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_z<1.59/**/*.fits\", recursive=True))):\n",
    "    try:\n",
    "        with fits.open(confirmed_single, memmap = False) as hdul:\n",
    "            img = hdul[1].data\n",
    "            img = crop_center(img, 94, 94)\n",
    "            rotation_AGN_candidate_images.append(img)\n",
    "            name = confirmed_single.split('/')[-1].split('.fits')[0]\n",
    "            rotation_AGN_candidate_names.append(name)\n",
    "            location = northern_df[northern_df['Quasar name'] == name]\n",
    "            if not location.empty:\n",
    "                ra = location.iloc[0]['ra']\n",
    "                dec = location.iloc[0]['dec']\n",
    "                rotation_ra_list.append(ra)\n",
    "                rotation_dec_list.append(dec)\n",
    "            else:\n",
    "                print(\"ERROR: cannot find quasar in northern sky dataframe\")\n",
    "    except OSError:\n",
    "        print(f\"file {confirmed_single} seems to be corrupted or cannot be opened, moving on...\")\n",
    "\n",
    "#filepath = \"Rotated_AGN_diff_fltrs/HSC_I/\"\n",
    "filepath = \"Randomized_Rotated_AGN_diff_fltrs/HSC_G/\"\n",
    "\n",
    "fltr = 'HSC-G'\n",
    "for percentage in tqdm(percentages):\n",
    "    for angular_separation in tqdm(separations_distribution_1):\n",
    "        angular_separation = np.round(angular_separation, 2)\n",
    "        \n",
    "        rand_index = np.random.choice(len(rotation_AGN_candidate_names))\n",
    "        rand_quasar_name = rotation_AGN_candidate_names[rand_index]\n",
    "        rand_quasar_ra_start = rotation_ra_list[rand_index] + 0.67 * np.cos(0)/3600.0\n",
    "        rand_quasar_dec_start = rotation_dec_list[rand_index]\n",
    "        percentages = random.uniform(0.1, 1.0)\n",
    "        tables = reducedAmplitudeTestFile(rand_quasar_name, percentage, angular_separation, rand_quasar_ra_start, rand_quasar_dec_start, fltr, filepath = filepath)\n",
    "        #if rand_quasar_name == quasar_name_1:\n",
    "            #tables = reducedAmplitudeTestFile(rand_quasar_name, percentage, angular_separation, ra_start_1, dec_start_1, fltr, filepath = filepath)\n",
    "        #else:\n",
    "            #tables = reducedAmplitudeTestFile(rand_quasar_name, percentage, angular_separation, ra_start_2, dec_start_2, fltr, filepath = filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a090f-05e0-4f78-b3ab-5374d98eacbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This script creates new 16asec x 16asec cutouts of QSO 101825.95+002057.9 to create dual AGN images that are upscaled to the correct size for the rest of the dataset\n",
    "percentages = np.arange(0.1, 1.0, 0.1)\n",
    "separations = np.arange(0.55, 2.5, 0.05)\n",
    "angle = np.arange(0.0, 359.0, 1.0)\n",
    "quasar_name = \"QSO_101823.39-012400.6\"\n",
    "ra_start = 154.59748850760818 + 0.55 * np.cos(0)/3600.0\n",
    "dec_start = -1.4001730205213287\n",
    "filepath = \"dual_AGN_training_data/Rotated_AGN_diff_fltrs/HSC_I/\"\n",
    "fltr = 'HSC-I'\n",
    "for ii, percentage in tqdm(enumerate(percentages)):\n",
    "    for j, angular_separation in enumerate(separations):\n",
    "        angular_separation = np.round(angular_separation, 2)\n",
    "        tables = reducedAmplitudeTestFile(quasar_name, percentage, angular_separation, ra_start, dec_start, fltr, filepath = filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9ab0e-5950-4fac-8fc0-20f94ced70f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This script will create cutout CSV and .txt files for rotated STAR objects. These objects will be stars chosen for their point-source resemblance\n",
    "# and their lack of background noise. These sources will be rotated around central AGN candidates to resemble a chance star-quasar alignment.\n",
    "star_name = \"STAR_105335.60-002157.8\"\n",
    "start_ra = 163.398361 + 0.55*np.cos(0)/3600.0\n",
    "start_dec = -0.366077\n",
    "star_filepath = \"Rotated_STAR_files/HSC_I/\"\n",
    "fltr = 'HSC-I'\n",
    "percentages = np.arange(0.1, 1.0, 0.1)\n",
    "separations = np.arange(0.55, 2.5, 0.05)\n",
    "for ii, percentage in tqdm(enumerate(percentages)):\n",
    "    for j, angular_separation in enumerate(separations):\n",
    "        angular_separation = np.round(angular_separation, 2)\n",
    "        tables = reducedAmplitudeTestFile(star_name, percentage, angular_separation, start_ra, start_dec, fltr, filepath = star_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bee62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def members(tarf):\n",
    "    \n",
    "    for i, member in enumerate(tarf.getmembers()):\n",
    "        if i == 0:\n",
    "            l = len(member.path.split(\"/\")[0]) + 1\n",
    "        \n",
    "        member.path = member.path[l:]\n",
    "        yield member\n",
    "        \n",
    "        \n",
    "#Function to actuall extract all the tarballs\n",
    "def extractor(args):\n",
    "    tar_file_prefix, write_path, i = args\n",
    "    \n",
    "    with tarfile.open(tar_file_prefix + \"download.tar\") as tarf:\n",
    "        tarf.extractall(write_path,members=members(tarf))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23da01ee-ea76-4100-bc76-b97082eae68a",
   "metadata": {},
   "source": [
    "##### extract = True\n",
    "separations = np.arange(1.95, 2.5, 0.05)\n",
    "for j, angular_separation in tqdm(enumerate(separations)):\n",
    "    total_files = 1404\n",
    "\n",
    "    i_s = np.arange(total_files)\n",
    "    prefix = \"Rotated_STAR_files/\" + str(\"HSC_I\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/downloaded_images\"\n",
    "    dwnldr_txt_filepath = \"Rotated_STAR_files/\" + str(\"HSC_I\")+ \"/downloader_files/downloader_for_\" + str(np.round(angular_separation, 2))+\"/\"\n",
    "    write_path = \"Rotated_STAR_files/\" + str(\"HSC_I\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/opened_images\"\n",
    "\n",
    "    #if extract is True:\n",
    "        #with mp.Pool() as p:\n",
    "            #exit_codes = list(tqdm(p.imap_unordered(cutout_requester,requester_args), total=len(i_s)))\n",
    "    total_threads = 2\n",
    "    #url = 'https://hsc-release.mtk.nao.ac.jp/das_cutout/pdr3/cgi-bin/cutout/'\n",
    "    url = 'https://hsc-release.mtk.nao.ac.jp/das_cutout/pdr3/cgi-bin/cutout'\n",
    "    if not exists(\"Rotated_STAR_files/\" + str(\"HSC_I\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/\"):\n",
    "        os.makedirs(\"Rotated_STAR_files/\" + str(\"HSC_I\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/\")\n",
    "    with requests.Session() as session:\n",
    "        args_cutout_requester = (session,dwnldr_txt_filepath,write_path,i_s)\n",
    "        cutout_requester(args_cutout_requester)\n",
    "    extractor_args = (write_path,prefix,i_s)\n",
    "    extractor(extractor_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c2314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract = True\n",
    "print(separations_distribution_1)\n",
    "for j, angular_separation in tqdm(enumerate(separations_distribution_1[47:])):\n",
    "    total_files = 1404\n",
    "\n",
    "    i_s = np.arange(total_files)\n",
    "    #prefix = \"Rotated_AGN_diff_fltrs/\" + str(\"HSC_G\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/downloaded_images\"\n",
    "    prefix = \"Randomized_Rotated_AGN_diff_fltrs/\" + str(\"HSC_G\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/downloaded_images\"\n",
    "    #dwnldr_txt_filepath = \"Rotated_AGN_diff_fltrs/\" + str(\"HSC_G\")+ \"/downloader_files/downloader_for_\" + str(np.round(angular_separation, 2))+\"/\"\n",
    "    dwnldr_txt_filepath = \"Randomized_Rotated_AGN_diff_fltrs/\" + str(\"HSC_G\")+ \"/downloader_files/downloader_for_\" + str(np.round(angular_separation, 2))+\"/\"\n",
    "    \n",
    "    #write_path = \"Rotated_AGN_diff_fltrs/\" + str(\"HSC_G\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/opened_images\"\n",
    "    write_path = \"Randomized_Rotated_AGN_diff_fltrs/\" + str(\"HSC_G\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/opened_images\"\n",
    "\n",
    "    #if extract is True:\n",
    "        #with mp.Pool() as p:\n",
    "            #exit_codes = list(tqdm(p.imap_unordered(cutout_requester,requester_args), total=len(i_s)))\n",
    "    total_threads = 2\n",
    "    #url = 'https://hsc-release.mtk.nao.ac.jp/das_cutout/pdr3/cgi-bin/cutout/'\n",
    "    url = 'https://hsc-release.mtk.nao.ac.jp/das_cutout/pdr3/cgi-bin/cutout'\n",
    "    if not exists(\"Randomized_Rotated_AGN_diff_fltrs/\" + str(\"HSC_G\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/\"):\n",
    "        os.makedirs(\"Randomized_Rotated_AGN_diff_fltrs/\" + str(\"HSC_G\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/\")\n",
    "    #with MultiPool(processes=2) as pool:\n",
    "        #with requests.Session() as session: #just to keep things clearn\n",
    "            #sessions = [session]*len(i_s)\n",
    "            #args_cutout_requester = list(zip(sessions,dwnldr_txt_filepath,write_path,i_s))\n",
    "            #exit_codes = list(tqdm(pool.imap_unordered(cutout_requester,args_cutout_requester), total = len(i_s)))\n",
    "    with requests.Session() as session:\n",
    "        args_cutout_requester = (session,dwnldr_txt_filepath,write_path,i_s)\n",
    "        cutout_requester(args_cutout_requester)\n",
    "    extractor_args = (write_path,prefix,i_s)\n",
    "    extractor(extractor_args)\n",
    "    #with open(\"Rotated_AGN_diff_fltrs/\" + str(fltr)+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/\" + \"download.tar\", 'wb') as out_file:\n",
    "\n",
    "    #extractor_args = list(zip(prefix,write_path,i_s))\n",
    "    #with mp.Pool() as p:\n",
    "        #exit_codes = list(tqdm(p.imap_unordered(extractor,extractor_args), total=len(i_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c770fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract = True\n",
    "for j, angular_separation in tqdm(enumerate(separations)):\n",
    "    total_files = 1404\n",
    "\n",
    "    i_s = np.arange(total_files)\n",
    "    prefix = \"dual_AGN_training_data/Rotated_AGN_diff_fltrs/\" + str(\"HSC_I\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/downloaded_images\"\n",
    "    dwnldr_txt_filepath = \"dual_AGN_training_data/Rotated_AGN_diff_fltrs/\" + str(\"HSC_I\")+ \"/downloader_files/downloader_for_\" + str(np.round(angular_separation, 2))+\"/\"\n",
    "    write_path = \"dual_AGN_training_data/Rotated_AGN_diff_fltrs/\" + str(\"HSC_I\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/opened_images\"\n",
    "\n",
    "    #if extract is True:\n",
    "        #with mp.Pool() as p:\n",
    "            #exit_codes = list(tqdm(p.imap_unordered(cutout_requester,requester_args), total=len(i_s)))\n",
    "    total_threads = 2\n",
    "    #url = 'https://hsc-release.mtk.nao.ac.jp/das_cutout/pdr3/cgi-bin/cutout/'\n",
    "    url = 'https://hsc-release.mtk.nao.ac.jp/das_cutout/pdr3/cgi-bin/cutout'\n",
    "    if not exists(\"dual_AGN_training_data/Rotated_AGN_diff_fltrs/\" + str(\"HSC_I\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/\"):\n",
    "        os.makedirs(\"dual_AGN_training_data/Rotated_AGN_diff_fltrs/\" + str(\"HSC_I\")+ \"/downloaded_images/\" + str(np.round(angular_separation, 2))+\"_arcsecond_separations/\")\n",
    "    with requests.Session() as session:\n",
    "        args_cutout_requester = (session,dwnldr_txt_filepath,write_path,i_s)\n",
    "        cutout_requester(args_cutout_requester)\n",
    "    extractor_args = (write_path,prefix,i_s)\n",
    "    extractor(extractor_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e0df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [\"HSC-G\", \"HSC-R\", \"HSC-Z\", \"HSC-Y\", \"HSC-I\"]\n",
    "for fltr in tqdm(bands):\n",
    "    print(fltr)\n",
    "    total_files = 100000\n",
    "    north_filepath_prefix = \"entire_bands/\" + str(fltr) + \"/northern_sky/\"\n",
    "    fall_filepath_prefix = \"entire_bands/\" + str(fltr) + \"/fall_equatorial/\"\n",
    "    spring_filepath_prefix = \"entire_bands/\" + str(fltr) + \"/spring_equatorial/\"\n",
    "    i_s = np.arange(total_files)\n",
    "    \n",
    "    north_prefix = north_filepath_prefix + \"downloaded_images/\"\n",
    "    north_dwnldr_txt_filepath = north_filepath_prefix\n",
    "    north_write_path = north_filepath_prefix + \"downloaded_images/opened_images\"\n",
    "    \n",
    "    spring_prefix = spring_filepath_prefix + \"downloaded_images/\"\n",
    "    spring_dwnldr_txt_filepath = spring_filepath_prefix\n",
    "    spring_write_path = spring_filepath_prefix + \"downloaded_images/opened_images\"\n",
    "    \n",
    "    fall_prefix = fall_filepath_prefix + \"downloaded_images/\"\n",
    "    fall_dwnldr_txt_filepath = fall_filepath_prefix\n",
    "    fall_write_path = fall_filepath_prefix + \"downloaded_images/opened_images\"\n",
    "    \n",
    "    \"\"\"if not exists(fall_prefix) and not exists(fall_dwnldr_txt_filepath) and not exists(fall_write_path):\n",
    "        os.makedirs(fall_prefix)\n",
    "        os.makedirs(fall_dwnldr_txt_filepath)\n",
    "        os.makedirs(fall_write_path)\n",
    "    if not exists(spring_prefix) and not exists(spring_dwnldr_txt_filepath) and not exists(spring_write_path):\n",
    "        os.makedirs(spring_prefix)\n",
    "        os.makedirs(spring_dwnldr_txt_filepath)\n",
    "        os.makedirs(spring_write_path)\n",
    "    if not exists(north_prefix) and not exists(north_dwnldr_txt_filepath) and not exists(north_write_path):\n",
    "        os.makedirs(north_prefix)\n",
    "        os.makedirs(north_dwnldr_txt_filepath)\n",
    "        os.makedirs(north_write_path)\"\"\"\n",
    "    write_band_download_cutouts(quasarNameList_fall, quasarRAList_fall, quasarDecList_fall, fltr, fall_dwnldr_txt_filepath, fall_filepath_prefix, 0.0)\n",
    "    write_band_download_cutouts(quasarNameList_spring, quasarRAList, quasarDecList, fltr, spring_dwnldr_txt_filepath, spring_filepath_prefix, 0.0)\n",
    "    write_band_download_cutouts(quasarNameList_north, quasarRAList_north, quasarDecList_north, fltr, north_dwnldr_txt_filepath, north_filepath_prefix, 0.0)\n",
    "\n",
    "    total_threads = 2\n",
    "    url = 'https://hsc-release.mtk.nao.ac.jp/das_cutout/pdr3/cgi-bin/cutout/'\n",
    "    with requests.Session() as session:\n",
    "        fall_args_cutout_requester = (session,fall_dwnldr_txt_filepath,fall_prefix,i_s)\n",
    "        spring_args_cutout_requester = (session,spring_dwnldr_txt_filepath,spring_prefix,i_s)\n",
    "        north_args_cutout_requester = (session,north_dwnldr_txt_filepath,north_prefix,i_s)\n",
    "        print(\"Fall cutouts\")\n",
    "        cutout_requester(fall_args_cutout_requester)\n",
    "        fall_extractor_args = (fall_write_path,fall_prefix,i_s)\n",
    "        extractor(fall_extractor_args)\n",
    "        \n",
    "        print(\"Spring cutouts\")\n",
    "        cutout_requester(spring_args_cutout_requester)\n",
    "        spring_extractor_args = (spring_write_path,spring_prefix,i_s)\n",
    "        extractor(spring_extractor_args)\n",
    "        \n",
    "        print(\"North cutouts\")\n",
    "        cutout_requester(north_args_cutout_requester)\n",
    "        spring_extractor_args = (north_write_path,north_prefix,i_s)\n",
    "        extractor(north_extractor_args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ab8dc-bc38-4933-aaf5-6b08b41d9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb9b2c-7f03-4a74-812e-056df8188d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single_AGN_HSC_I_filepath = \"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_AGN/\"\n",
    "single_AGN_HSC_I_filepath = \"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_z<1.59/z<1.59/\"\n",
    "single_HSCI = []\n",
    "single_HSCI_filenames = []\n",
    "for image in tqdm(glob.glob(single_AGN_HSC_I_filepath+\"*.fits\")):\n",
    "    single_HSCI_filenames.append(image)\n",
    "    with fits.open(image) as hdul:\n",
    "        img = hdul[1].data\n",
    "        img = crop_center(img, 94, 94)\n",
    "        single_HSCI.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bfec57-1602-4a11-a3b2-99b4232044b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images_with_buttons(single_HSCI, single_HSCI_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "441bde4d-1420-4cb7-9084-6e21bdb696f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8299347031448ae906e7cbde33d7b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: File may have been truncated: actual file length (38224) is smaller than the expected size (40320) [astropy.io.fits.file]\n"
     ]
    }
   ],
   "source": [
    "dual_AGN_filepath = \"data_preprocessing/training_datasets/dual_AGN_datasets/train_data/\"\n",
    "dual_HSCG_train = []\n",
    "dual_HSCG_train_filenames = []\n",
    "for image in tqdm(glob.glob(dual_AGN_filepath+\"*.fits\")):\n",
    "    dual_HSCG_train_filenames.append(image)\n",
    "    with fits.open(image) as hdul:\n",
    "        img = hdul[0].data\n",
    "        img = crop_center(img, 94, 94)\n",
    "        dual_HSCG_train.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df945770-7db8-44b5-b385-ca92358f9784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7858cbb1b5e6413a963d8b9ceeaf6880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(Button(description='Image 1', style=ButtonStyle()), Output()), layout=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_images_with_buttons(dual_HSCG_train, dual_HSCG_train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e95b2-fa0b-4e68-b70f-580d503e9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dual_AGN_filepath = \"data_preprocessing/training_datasets/dual_AGN_datasets/new_train_data/\"\n",
    "dual_AGN_filepath = \"hsc_2626_test_HSC-G/\"\n",
    "dual_HSCG = []\n",
    "dual_HSCG_filenames = []\n",
    "for image in tqdm(glob.glob(dual_AGN_filepath+\"*.fits\")):\n",
    "    dual_HSCG_filenames.append(image)\n",
    "    with fits.open(image, memmap = False) as hdul:\n",
    "        img = hdul[1].data\n",
    "        img = crop_center(img, 94, 94)\n",
    "        dual_HSCG.append(img)\n",
    "display_images_with_buttons(dual_HSCG, dual_HSCG_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63b1bd-c563-4993-bf92-ec76a9a5bfc6",
   "metadata": {},
   "source": [
    "Create randomized dual AGN images using the `DatasetCreator` object and API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d2111e1-d724-4c79-8027-5cd09f58db87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50102b1eb33c461dacaec0c08f67f132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0b11bda68d43fb8412ce8c539d449f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f610c8f8734deb8ca4fa8d8884ae81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m hdu \u001b[38;5;241m=\u001b[39m fits\u001b[38;5;241m.\u001b[39mPrimaryHDU(convolved_image)\n\u001b[1;32m     36\u001b[0m hdul \u001b[38;5;241m=\u001b[39m fits\u001b[38;5;241m.\u001b[39mHDUList([hdu])\n\u001b[0;32m---> 37\u001b[0m \u001b[43mhdul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfits_filepath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mii\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_with_AGN_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mj\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_randomized.fits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/hdu/hdulist.py:1043\u001b[0m, in \u001b[0;36mHDUList.writeto\u001b[0;34m(self, fileobj, output_verify, overwrite, checksum)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m hdu \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1042\u001b[0m             hdu\u001b[38;5;241m.\u001b[39m_prewriteto(checksum\u001b[38;5;241m=\u001b[39mchecksum)\n\u001b[0;32m-> 1043\u001b[0m             \u001b[43mhdu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writeto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdulist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m             hdu\u001b[38;5;241m.\u001b[39m_postwriteto()\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/hdu/base.py:722\u001b[0m, in \u001b[0;36m_BaseHDU._writeto\u001b[0;34m(self, fileobj, inplace, copy)\u001b[0m\n\u001b[1;32m    719\u001b[0m     dirname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _free_space_check(\u001b[38;5;28mself\u001b[39m, dirname):\n\u001b[0;32m--> 722\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writeto_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/hdu/base.py:728\u001b[0m, in \u001b[0;36m_BaseHDU._writeto_internal\u001b[0;34m(self, fileobj, inplace, copy)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new:\n\u001b[1;32m    727\u001b[0m     header_offset, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writeheader(fileobj)\n\u001b[0;32m--> 728\u001b[0m     data_offset, data_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writedata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# Set the various data location attributes on newly-written HDUs\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new:\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/hdu/base.py:659\u001b[0m, in \u001b[0;36m_BaseHDU._writedata\u001b[0;34m(self, fileobj)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loaded \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_needs_rescale:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 659\u001b[0m         size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writedata_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# pad the FITS data block\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;66;03m# to avoid a bug in the lustre filesystem client, don't\u001b[39;00m\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# write zero-byte objects\u001b[39;00m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _pad_length(size) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/hdu/image.py:674\u001b[0m, in \u001b[0;36m_ImageBaseHDU._writedata_internal\u001b[0;34m(self, fileobj)\u001b[0m\n\u001b[1;32m    672\u001b[0m output\u001b[38;5;241m.\u001b[39mbyteswap(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m     \u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwritearray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    676\u001b[0m     output\u001b[38;5;241m.\u001b[39mbyteswap(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/file.py:432\u001b[0m, in \u001b[0;36m_File.writearray\u001b[0;34m(self, array)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 432\u001b[0m     \u001b[43m_array_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/util.py:616\u001b[0m, in \u001b[0;36m_array_to_file\u001b[0;34m(arr, outfile)\u001b[0m\n\u001b[1;32m    613\u001b[0m     chunksize \u001b[38;5;241m=\u001b[39m _WIN_WRITE_LIMIT \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m arr\u001b[38;5;241m.\u001b[39mitemsize\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# Just pass the whole array to the write routine\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# Write one chunk at a time for systems whose fwrite chokes on large\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# writes.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/util.py:591\u001b[0m, in \u001b[0;36m_array_to_file.<locals>.<lambda>\u001b[0;34m(a, f)\u001b[0m\n\u001b[1;32m    588\u001b[0m     seekable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfile(outfile) \u001b[38;5;129;01mand\u001b[39;00m seekable:\n\u001b[0;32m--> 591\u001b[0m     write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a, f: \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     write \u001b[38;5;241m=\u001b[39m _array_to_file_like\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#dataset_creation_session = DatasetCreator()\n",
    "#single_AGN_filepath_HSC_G = \"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_AGN/confirmed_single_z<1.59/\"\n",
    "#single_AGN_filepath_HSC_G = \"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_z<1.59/\"\n",
    "single_AGN_filepath_HSC_G = \"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_AGN/\"\n",
    "random_dual_AGN_filepath_HSC_G = \"Randomized_Rotated_AGN_diff_fltrs/HSC_G/downloaded_images/\"\n",
    "fits_filepath = \"data_preprocessing/training_datasets/dual_AGN_datasets/train_data/\"\n",
    "if not exists(fits_filepath):\n",
    "    os.makedirs(fits_filepath)\n",
    "single_AGN_images = []\n",
    "#for image_file in tqdm(glob.glob(single_AGN_filepath_HSC_G+\"**/*.fits\", recursive = True)):\n",
    "for image_file in tqdm(glob.glob(single_AGN_filepath_HSC_G+\"*.fits\")):\n",
    "    with fits.open(image_file, memmap=False) as hdul:\n",
    "        img = hdul[1].data\n",
    "        img = crop_center(img, 94, 94)\n",
    "        single_AGN_images.append(img)\n",
    "randomized_dual_images = []\n",
    "for image_file in tqdm(glob.glob(random_dual_AGN_filepath_HSC_G+\"**/*.fits\", recursive=True)):\n",
    "    try:\n",
    "        with fits.open(image_file, memmap=False) as hdul:\n",
    "            img = hdul[1].data\n",
    "            img = crop_center(img, 94, 94)\n",
    "            randomized_dual_images.append(img)\n",
    "    except OSError:\n",
    "        print(f\"file {image_file} is corrupted or otherwise cannot be opened, moving on...\")\n",
    "# Now create convolution of images\n",
    "#for ii, single_image in tqdm(enumerate(single_AGN_images)):\n",
    "percentages = np.random.uniform(0.2, 1.0, 80)\n",
    "for j, single_image in tqdm(enumerate(single_AGN_images)):\n",
    "    for percentage in percentages:\n",
    "        ii = np.random.choice(len(randomized_dual_images))\n",
    "        #single_image = np.random.choice(single_AGN_images)\n",
    "        #ingle_image = single_AGN_images[ii]\n",
    "        random_dual = randomized_dual_images[ii]\n",
    "        convolved_image = single_image + random_dual\n",
    "        hdu = fits.PrimaryHDU(convolved_image)\n",
    "        hdul = fits.HDUList([hdu])\n",
    "        hdul.writeto(f\"{fits_filepath}object{ii}_with_AGN_{j}_randomized.fits\" , overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc9849c5-c860-47c6-b0f7-e9c5fa01d401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e403e912b4ef4aea8416b6b62f990868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd9d27f7aae44488f0e4125dcec4053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1105ea74becd4bdf889adcf366be51bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m hdu \u001b[38;5;241m=\u001b[39m fits\u001b[38;5;241m.\u001b[39mPrimaryHDU(convolved_image)\n\u001b[1;32m     31\u001b[0m hdul \u001b[38;5;241m=\u001b[39m fits\u001b[38;5;241m.\u001b[39mHDUList([hdu])\n\u001b[0;32m---> 32\u001b[0m \u001b[43mhdul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfits_filepath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mii\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_with_AGN_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mj\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_randomized.fits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/hdu/hdulist.py:1032\u001b[0m, in \u001b[0;36mHDUList.writeto\u001b[0;34m(self, fileobj, output_verify, overwrite, checksum)\u001b[0m\n\u001b[1;32m   1028\u001b[0m mode \u001b[38;5;241m=\u001b[39m FILE_MODES[fileobj_mode(fileobj)] \u001b[38;5;28;01mif\u001b[39;00m isfile(fileobj) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mostream\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# This can accept an open file object that's open to write only, or in\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# append/update modes but only if the file doesn't exist.\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m fileobj \u001b[38;5;241m=\u001b[39m \u001b[43m_File\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m hdulist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfromfile(fileobj)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/file.py:218\u001b[0m, in \u001b[0;36m_File.__init__\u001b[0;34m(self, fileobj, mode, memmap, overwrite, cache, use_fsspec, fsspec_kwargs, decompress_in_memory)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_fileobj(fileobj, mode, overwrite)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fileobj, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_filelike(fileobj, mode, overwrite)\n",
      "File \u001b[0;32m~/.conda/envs/DualFinder/lib/python3.11/site-packages/astropy/io/fits/file.py:641\u001b[0m, in \u001b[0;36m_File._open_filename\u001b[0;34m(self, filename, mode, overwrite)\u001b[0m\n\u001b[1;32m    638\u001b[0m ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_read_compressed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, magic, mode, ext\u001b[38;5;241m=\u001b[39mext):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, IO_FITS_MODES[mode])\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose_on_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Make certain we're back at the beginning of the file\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# BZ2File does not support seek when the file is open for writing, but\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# when opening a file for write, bz2.BZ2File always truncates anyway.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#dataset_creation_session = DatasetCreator()\n",
    "#single_AGN_filepath_HSC_G = \"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_AGN/confirmed_single_z<1.59/\"\n",
    "galaxy_filepath_HSC_G = \"data_preprocessing/training_datasets/singleton_galaxies/train_data/\"\n",
    "random_dual_AGN_filepath_HSC_G = \"Randomized_Rotated_AGN_diff_fltrs/HSC_G/downloaded_images/\"\n",
    "fits_filepath = \"data_preprocessing/training_datasets/offset_AGN_datasets/train_data/\"\n",
    "if not exists(fits_filepath):\n",
    "    os.makedirs(fits_filepath)\n",
    "galaxy_images = []\n",
    "for image_file in tqdm(glob.glob(galaxy_filepath_HSC_G+\"**/*.fits\", recursive = True)):\n",
    "    with fits.open(image_file, memmap=False) as hdul:\n",
    "        img = hdul[1].data\n",
    "        img = crop_center(img, 94, 94)\n",
    "        galaxy_images.append(img)\n",
    "randomized_dual_images = []\n",
    "for image_file in tqdm(glob.glob(random_dual_AGN_filepath_HSC_G+\"**/*.fits\", recursive=True)):\n",
    "    try:\n",
    "        with fits.open(image_file, memmap=False) as hdul:\n",
    "            img = hdul[1].data\n",
    "            img = crop_center(img, 94, 94)\n",
    "            randomized_dual_images.append(img)\n",
    "    except OSError:\n",
    "        print(f\"file {image_file} is corrupted or otherwise cannot be opened, moving on...\")\n",
    "# Now create convolution of images\n",
    "#for ii, single_image in tqdm(enumerate(single_AGN_images)):\n",
    "for j, random_dual in tqdm(enumerate(randomized_dual_images)):\n",
    "    ii = np.random.choice(len(galaxy_images))\n",
    "    #single_image = np.random.choice(single_AGN_images)\n",
    "    single_image = galaxy_images[ii]\n",
    "    convolved_image = single_image + random_dual\n",
    "    hdu = fits.PrimaryHDU(convolved_image)\n",
    "    hdul = fits.HDUList([hdu])\n",
    "    hdul.writeto(f\"{fits_filepath}object{ii}_with_AGN_{j}_randomized.fits\" , overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376f404-b3f5-462c-bd92-2d907ae70ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_creation_session = DatasetCreator()\n",
    "single_AGN_filepath_HSC_I = \"data_preprocessing/training_datasets/single_AGN_datasets/HSC-I/confirmed_single_AGN/\"\n",
    "rotated_star_filepath_HSC_I = \"Rotated_STAR_files/HSC_I/downloaded_images/\"\n",
    "fits_star_filepath_HSC_I = \"data_preprocessing/training_datasets/stellar_dataset/HSC-I/train_data/\"\n",
    "dataset_creation_session.extract_single_galaxies(single_AGN_filepath_HSC_I)\n",
    "dataset_creation_session.extract_rotated_AGN(rotated_star_filepath_HSC_I)\n",
    "dataset_creation_session.create_convolution(fits_filepath = fits_star_filepath_HSC_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954e137-1812-4f84-b9b9-715779ad6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_creation_session = DatasetCreator()\n",
    "singleton_galaxy_HSC_I = \"data_preprocessing/training_datasets/singleton_galaxies/HSC-I/\"\n",
    "rotated_AGN_galaxy_HSC_I = \"Rotated_AGN_diff_fltrs/HSC_I/downloaded_images/\"\n",
    "fits_offset_filepath_HSC_I = \"data_preprocessing/training_datasets/offset_AGN_datasets/HSC-I/train_data/\"\n",
    "dataset_creation_session.extract_single_galaxies(singleton_galaxy_HSC_I)\n",
    "dataset_creation_session.extract_rotated_AGN(rotated_AGN_galaxy_HSC_I)\n",
    "dataset_creation_session.create_convolution(fits_filepath = fits_offset_filepath_HSC_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ce294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pwd\n",
    "dataset_creation_session = DatasetCreator()\n",
    "single_AGN_filepath = \"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_AGN/\"\n",
    "#single_AGN_filepath = \"data_preprocessing/training_datasets/single_AGN_datasets/HSC-I/confirmed_single_AGN/\"\n",
    "rotated_AGN_filepath = \"dual_AGN_training_data/Rotated_AGN_diff_fltrs/HSC_G/downloaded_images/\"\n",
    "#rotated_AGN_filepath = \"dual_AGN_training_data/Rotated_AGN_diff_fltrs/HSC_I/downloaded_images/\"\n",
    "fits_dual_AGN_filepath = \"data_preprocessing/training_datastes/dual_AGN_datasets/new_train_data/\"\n",
    "#fits_dual_AGN_filepath = \"data_preprocessing/training_datasets/dual_AGN_datasets/HSC-I/train_data/\"\n",
    "dataset_creation_session.extract_single_galaxies(single_AGN_filepath)\n",
    "dataset_creation_session.extract_rotated_AGN(rotated_AGN_filepath)\n",
    "#dataset_creation_session.extract_single_point_sources(\n",
    "dataset_creation_session.create_convolution(fits_filepath = fits_dual_AGN_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d02d4b-7f6a-4758-aaeb-1e20d6e936d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates offset AGN images with downloaded confirmed non-merger galaxies. \n",
    "dataset_creation_session = DatasetCreator()\n",
    "#single_AGN_filepath = \"data_preprocessing/training_datasets/single_AGN_datasets/confirmed_single_AGN/\"\n",
    "single_galaxy_filepath = \"../Rotated_AGN_diff_fltrs/HSC_G/filtered_ds/\"\n",
    "#rotated_AGN_filepath = \"data_preprocessing/training_datasets/dual_AGN_datasets/Rotated_AGN_diff_fltrs/HSC_G/downloaded_images/\"\n",
    "rotated_AGN_filepath = \n",
    "fits_dual_AGN_filepath = \"data_preprocessing/training_datasets/offset_AGN_datasets/train_data/\"\n",
    "dataset_creation_session.extract_single_galaxies(single_galaxy_filepath)\n",
    "dataset_creation_session.extract_rotated_AGN(rotated_AGN_filepath)\n",
    "#dataset_creation_session.extract_single_point_sources(\n",
    "dataset_creation_session.create_convolution(fits_dual_AGN_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c15cf-6dac-45b5-b595-5e3d430448f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272d827-109e-4be7-8806-919e4334ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_training_images = []\n",
    "for ii, image in tqdm(enumerate(glob.glob(\"data_preprocessing/training_datasets/offset_AGN_datasets/HSC-I/train_data/*.fits\"))):\n",
    "    try:\n",
    "        with fits.open(image, memmap = False) as hdul:\n",
    "            img = hdul[0].data\n",
    "            if np.shape(img) != (94, 94):\n",
    "                print(f\"ERROR: image has shape {np.shape(img)}\")\n",
    "            offset_training_images.append(img)\n",
    "        #print(f\" {ii} images were successfully loaded, {image} is corrupted/could not be loaded\")\n",
    "    except OSError:\n",
    "        print(f\" {ii} images were successfully loaded, {image} is corrupted/could not be loaded\")\n",
    "        continue\n",
    "random_offset_selection = np.random.choice(len(offset_training_images), size = 25)\n",
    "fig, axes = plt.subplots(5, 5, figsize = (20,20))\n",
    "counter = 0\n",
    "for row in range(5):\n",
    "    for col in range(5):\n",
    "        img = offset_training_images[random_offset_selection[counter]]\n",
    "        axes[row][col].imshow(img, vmin = np.percentile(img, 1), vmax = np.percentile(img, 99))\n",
    "        counter += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c34e8fa-48e4-40ed-a213-f77ea5c622c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_training_images = []\n",
    "for ii, image in tqdm(enumerate(glob.glob(\"data_preprocessing/training_datasets/dual_AGN_datasets/train_data/*.fits\"))):\n",
    "    try:\n",
    "        with fits.open(image, memmap = False) as hdul:\n",
    "            img = hdul[0].data\n",
    "            if np.shape(img) != (94, 94):\n",
    "                print(f\"ERROR: image has shape {np.shape(img)}\")\n",
    "            dual_training_images.append(img)\n",
    "    except OSError:\n",
    "        print(f\"{image} cannot be opened\")\n",
    "        #print(f\" {ii} images were successfully loaded, {image} is corrupted/could not be loaded\")\n",
    "    #except OSError:\n",
    "random_dual_selection = np.random.choice(len(dual_training_images), size = 25)\n",
    "fig, axes = plt.subplots(5, 5, figsize = (20,20))\n",
    "counter = 0\n",
    "for row in range(5):\n",
    "    for col in range(5):\n",
    "        img = dual_training_images[random_dual_selection[counter]]\n",
    "        axes[row][col].imshow(img, vmin = np.percentile(img, 1), vmax = np.percentile(img, 99))\n",
    "        counter += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02beb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
